# Random Forest vs XGBoost â€“ Ensemble Learning Tutorial

## Overview

This tutorial provides a comparative analysis of two powerful ensemble methods: *Random Forest* and *XGBoost*. It explains the underlying mechanisms of both algorithms, evaluates their performance on a built-in classification dataset, and highlights strengths and trade-offs between them.

## Learning Objectives

- Understand how Random Forest and XGBoost operate
- Implement and tune both models using scikit-learn and xgboost
- Compare performance using accuracy, F1 score, and confusion matrix
- Visualize feature importance and model behavior

## Dataset

- Built-in dataset from sklearn.datasets
- Suitable for classification tasks
- Preprocessing steps are included in the notebook

## How to Run

1. Ensure you have scikit-learn, xgboost, matplotlib, seaborn, pandas, and numpy installed.
2. Open the notebook ram&XGB.ipynb in Jupyter or VS Code.
3. Run the notebook from top to bottom to reproduce all results and plots.

## File Structure
